{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21031211508/anaconda3/envs/torch_zhu/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/21031211508/anaconda3/envs/torch_zhu/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, precision_score, roc_auc_score\n",
    "from sklearn import metrics\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import scipy.sparse as sp\n",
    "from torch.optim import Adam\n",
    "\n",
    "from layers import SinkhornDistance\n",
    "from model_peryton2 import *\n",
    "from utils import *\n",
    "from get_sim import *\n",
    "import args\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "## peryton_sign1  without wloss and aux, crossentropy -> MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of microbes and diseases (1396, 43)\n",
      "the number of associations -88.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# paprameters\n",
    "\n",
    "random.seed(1)\n",
    "k1 = 140  # mic\n",
    "k2 = 5  # dis\n",
    "A = pd.read_csv('./peryton/sign_final_ass.csv', index_col=0).to_numpy()\n",
    "\n",
    "print(\"the number of microbes and diseases\", A.shape)\n",
    "print(\"the number of associations\", sum(sum(A)))\n",
    "x, y = A.shape\n",
    "score_matrix = np.zeros([x, y])\n",
    "samples = get_all_the_samples(A)\n",
    "\n",
    "# sim_m, sim_d = get_sim_3ss_1ms_peryton(A, k1, k2)\n",
    "sim_m, sim_d = np.load('./peryton/sim_m_sign.npy'), np.load('./peryton/sim_d_sign.npy')\n",
    "sim_m_0 = set_digo_zero(sim_m, 0)\n",
    "sim_d_0 = set_digo_zero(sim_d, 0)\n",
    "\n",
    "features_m, features_d = get_features_A(A)\n",
    "features_m, features_d = sparse_to_tuple(sp.coo_matrix(features_m)), sparse_to_tuple(sp.coo_matrix(features_d))\n",
    "# features_m = sparse_to_tuple(sp.coo_matrix(A))\n",
    "# features_d = sparse_to_tuple(sp.coo_matrix(A.transpose()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGAE3_2(\n",
      "  (conv1): GraphConv(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (w): Linear(in_features=86, out_features=256, bias=False)\n",
      "  )\n",
      "  (conv2): GraphConv(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (w): Linear(in_features=256, out_features=128, bias=False)\n",
      "  )\n",
      "  (meanGCN1): GraphConv(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (w): Linear(in_features=128, out_features=64, bias=False)\n",
      "  )\n",
      "  (logstdGCN1): GraphConv(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (w): Linear(in_features=128, out_features=64, bias=False)\n",
      "  )\n",
      "  (meanGCN2): GraphConv(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (w): Linear(in_features=128, out_features=32, bias=False)\n",
      "  )\n",
      "  (logstdGCN2): GraphConv(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (w): Linear(in_features=128, out_features=32, bias=False)\n",
      "  )\n",
      "  (meanGCN3): GraphConv(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (w): Linear(in_features=128, out_features=16, bias=False)\n",
      "  )\n",
      "  (logstdGCN3): GraphConv(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (w): Linear(in_features=128, out_features=16, bias=False)\n",
      "  )\n",
      "  (transform_z): Linear(in_features=112, out_features=112, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): LeakyReLU(negative_slope=0.1)\n",
      "  (aux1): Linear(in_features=112, out_features=256, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (relu1): LeakyReLU(negative_slope=0.1)\n",
      "  (aux2): Linear(in_features=112, out_features=128, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (relu2): LeakyReLU(negative_slope=0.1)\n",
      ")\n",
      "Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0.0001\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "adj_norm = preprocess_graph(sim_m_0)\n",
    "\n",
    "adj = sim_m_0\n",
    "# pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "# norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "pos_weight = 25\n",
    "norm = 0.5\n",
    "\n",
    "sim_m_0 = sp.coo_matrix(sim_m_0)\n",
    "sim_m_0.eliminate_zeros()\n",
    "adj_label = sim_m_0 + sp.eye(sim_m_0.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].T),\n",
    "                                    torch.FloatTensor(adj_norm[1]),\n",
    "                                    torch.Size(adj_norm[2])).to(device)\n",
    "adj_label = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].T),\n",
    "                                     torch.FloatTensor(adj_label[1]),\n",
    "                                     torch.Size(adj_label[2])).to_dense().to(device)\n",
    "features = torch.sparse.FloatTensor(torch.LongTensor(features_m[0].T),\n",
    "                                    torch.FloatTensor(features_m[1]),\n",
    "                                    torch.Size(features_m[2])).to(device)\n",
    "\n",
    "weight_mask = adj_label.view(-1) == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0)).to(device)\n",
    "# weight_tensor[weight_mask] = pos_weight\n",
    "weight_tensor[weight_mask] = 25\n",
    "\n",
    "# init model and optimizer\n",
    "model = VGAE3_2()\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=args.learning_rate)\n",
    "optimizer = Adam(model.parameters(), lr=args.learning_rate, weight_decay=1e-4)\n",
    "print(\"Optimizer\", optimizer)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 500, gamma=0.8, last_epoch=-1)\n",
    "\n",
    "sinkhorn = SinkhornDistance(eps=0.1, max_iter=100, device=device)\n",
    "\n",
    "tra_auc, tra_ap, tra_l, v_auc, v_ap = [], [], [], [], []\n",
    "tra_acc, tra_baseloss, tra_aux1, tra_aux2 = [], [], [], []\n",
    "train_wloss, tra_kl = [], []\n",
    "tra_r2, tra_RMSE = [], []\n",
    "\n",
    "min_loss = 10.\n",
    "min_mse = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Epoch: 0001 base_loss= 18.34065 train_loss= 18.34065 train_r2= -730.63384 train_RMSE= 6.05650 time= 2.15425\n",
      "--------------------------------\n",
      "Epoch: 0002 base_loss= 18.13314 train_loss= 18.13314 train_r2= -722.35582 train_RMSE= 6.02214 time= 2.09686\n",
      "--------------------------------\n",
      "Epoch: 0003 base_loss= 17.26904 train_loss= 17.26904 train_r2= -687.88540 train_RMSE= 5.87690 time= 1.95547\n",
      "--------------------------------\n",
      "Epoch: 0004 base_loss= 16.49404 train_loss= 16.49404 train_r2= -656.96882 train_RMSE= 5.74351 time= 2.23896\n",
      "--------------------------------\n",
      "Epoch: 0005 base_loss= 15.87214 train_loss= 15.87214 train_r2= -632.15958 train_RMSE= 5.63419 time= 2.55503\n",
      "--------------------------------\n",
      "Epoch: 0006 base_loss= 14.93691 train_loss= 14.93691 train_r2= -594.85052 train_RMSE= 5.46567 time= 2.08512\n",
      "--------------------------------\n",
      "Epoch: 0007 base_loss= 14.42729 train_loss= 14.42729 train_r2= -574.51950 train_RMSE= 5.37161 time= 1.95816\n",
      "--------------------------------\n",
      "Epoch: 0008 base_loss= 13.69498 train_loss= 13.69498 train_r2= -545.30396 train_RMSE= 5.23350 time= 1.98130\n",
      "--------------------------------\n",
      "Epoch: 0009 base_loss= 13.14958 train_loss= 13.14958 train_r2= -523.54414 train_RMSE= 5.12821 time= 1.91610\n",
      "--------------------------------\n",
      "Epoch: 0010 base_loss= 12.41702 train_loss= 12.41702 train_r2= -494.31733 train_RMSE= 4.98329 time= 2.24082\n",
      "--------------------------------\n",
      "Epoch: 0011 base_loss= 11.52941 train_loss= 11.52941 train_r2= -458.90428 train_RMSE= 4.80185 time= 1.96387\n",
      "--------------------------------\n",
      "Epoch: 0012 base_loss= 10.76896 train_loss= 10.76896 train_r2= -428.56233 train_RMSE= 4.64075 time= 2.19638\n",
      "--------------------------------\n",
      "Epoch: 0013 base_loss= 9.92872 train_loss= 9.92872 train_r2= -395.03502 train_RMSE= 4.45596 time= 2.03691\n",
      "--------------------------------\n",
      "Epoch: 0014 base_loss= 9.16048 train_loss= 9.16048 train_r2= -364.37796 train_RMSE= 4.28002 time= 1.94198\n",
      "--------------------------------\n",
      "Epoch: 0015 base_loss= 8.24416 train_loss= 8.24416 train_r2= -327.81031 train_RMSE= 4.06020 time= 1.88097\n",
      "--------------------------------\n",
      "Epoch: 0016 base_loss= 7.46806 train_loss= 7.46806 train_r2= -296.83238 train_RMSE= 3.86421 time= 1.98292\n",
      "--------------------------------\n",
      "Epoch: 0017 base_loss= 6.83820 train_loss= 6.83820 train_r2= -271.68314 train_RMSE= 3.69746 time= 1.98422\n",
      "--------------------------------\n",
      "Epoch: 0018 base_loss= 5.86639 train_loss= 5.86639 train_r2= -232.88754 train_RMSE= 3.42435 time= 1.94501\n",
      "--------------------------------\n",
      "Epoch: 0019 base_loss= 5.15193 train_loss= 5.15193 train_r2= -204.35034 train_RMSE= 3.20865 time= 1.94809\n",
      "--------------------------------\n",
      "Epoch: 0020 base_loss= 4.35729 train_loss= 4.35729 train_r2= -172.60657 train_RMSE= 2.95024 time= 2.23779\n"
     ]
    }
   ],
   "source": [
    "\n",
    "suffix = 'peryton_sign3'\n",
    "for epoch in range(20):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    A_pred, x1, x2, z = model(adj_norm, features)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # loss = base_loss = norm * F.binary_cross_entropy(A_pred.view(-1), adj_label.view(-1), weight=weight_tensor)\n",
    "    loss = base_loss = norm * F.mse_loss(A_pred.view(-1), adj_label.view(-1))\n",
    "    tra_baseloss.append(loss.item())\n",
    "\n",
    "    kl_divergence = 0.5 / A_pred.size(0) * (\n",
    "            1 + 2 * model.logstd - model.mean ** 2 - torch.exp(model.logstd) ** 2).sum(1).mean() \n",
    "    tra_kl.append(-kl_divergence.item())\n",
    "    loss -= kl_divergence\n",
    "\n",
    "    # wasser_loss = 0.5 / A_pred.size(0) * sinkhorn(z, torch.randn_like(z))[0]\n",
    "    # train_wloss.append(wasser_loss.item())\n",
    "    # loss += wasser_loss * w_w\n",
    "    # loss += wasser_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    r2 = r2_score(adj_label.cpu().detach().numpy().reshape(-1), A_pred.cpu().detach().numpy().reshape(-1))\n",
    "    RMSE = np.sqrt(mean_squared_error(adj_label.cpu().detach().numpy().reshape(-1), A_pred.cpu().detach().numpy().reshape(-1)))\n",
    "    tra_r2.append(r2)\n",
    "    tra_RMSE.append(RMSE)\n",
    "\n",
    "    tra_l.append(loss.item())\n",
    "    print('--------------------------------')\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"base_loss=\", \"{:.5f}\".format(base_loss.item()),\n",
    "          \"train_loss=\", \"{:.5f}\".format(loss.item()),\n",
    "          \"train_r2=\", \"{:.5f}\".format(r2), \"train_RMSE=\", \"{:.5f}\".format(RMSE),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "    if RMSE < min_mse:\n",
    "        min_mse = RMSE\n",
    "        state = {'model': model.state_dict(),\n",
    "                 'epoch': epoch,\n",
    "                 'min_mse': min_mse,\n",
    "                 }\n",
    "        torch.save(state, './sign/models/M_mse_256_128_64_5000_' + suffix + '.pth')\n",
    "\n",
    "model.eval()\n",
    "save_dic = {'train_loss': tra_l,\n",
    "            'train_baseloss': tra_baseloss,\n",
    "            'train_aux1': tra_aux1,\n",
    "            'train_aux2': tra_aux2,\n",
    "            'train_wloss': train_wloss,\n",
    "            'train_kl': tra_kl,\n",
    "            'train_r2': tra_r2,\n",
    "            'train_RMSE': tra_RMSE,\n",
    "            }\n",
    "np.save('./sign/metrics/M_mse_256_128_64_5000_' + suffix + '.npy', save_dic)\n",
    "model_ = VGAE3_2()\n",
    "model_.to(device)\n",
    "model_.load_state_dict(torch.load('./sign/models/M_mse_256_128_64_5000_' + suffix + '.pth')['model'])\n",
    "model_.eval()\n",
    "out_m = model_(adj_norm, features)\n",
    "\n",
    "latent_m = out_m[-1].cpu().detach().numpy()\n",
    "np.save('./sign/latent/latent_m_' + suffix + '.npy', latent_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0.0001\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "adj_norm2 = preprocess_graph(sim_d_0)\n",
    "\n",
    "adj2 = sim_d_0\n",
    "# pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "# norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "pos_weight2 = 25\n",
    "norm2 = 0.5\n",
    "\n",
    "sim_d_0 = sp.coo_matrix(sim_d_0)\n",
    "sim_d_0.eliminate_zeros()\n",
    "adj_label2 = sim_d_0 + sp.eye(sim_d_0.shape[0])\n",
    "adj_label2 = sparse_to_tuple(adj_label2)\n",
    "\n",
    "adj_norm2 = torch.sparse.FloatTensor(torch.LongTensor(adj_norm2[0].T),\n",
    "                                     torch.FloatTensor(adj_norm2[1]),\n",
    "                                     torch.Size(adj_norm2[2])).to(device)\n",
    "adj_label2 = torch.sparse.FloatTensor(torch.LongTensor(adj_label2[0].T),\n",
    "                                      torch.FloatTensor(adj_label2[1]),\n",
    "                                      torch.Size(adj_label2[2])).to_dense().to(device)\n",
    "features2 = torch.sparse.FloatTensor(torch.LongTensor(features_d[0].T),\n",
    "                                     torch.FloatTensor(features_d[1]),\n",
    "                                     torch.Size(features_d[2])).to(device)\n",
    "\n",
    "weight_mask2 = adj_label2.view(-1) == 1\n",
    "weight_tensor2 = torch.ones(weight_mask2.size(0)).to(device)\n",
    "# weight_tensor[weight_mask] = pos_weight\n",
    "weight_tensor2[weight_mask2] = 25\n",
    "\n",
    "# init model and optimizer\n",
    "model2 = VGAE4_2()\n",
    "model2.to(device)\n",
    "\n",
    "# optimizer2 = Adam(model2.parameters(), lr=0.001)\n",
    "optimizer2 = Adam(model2.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "print(\"Optimizer\", optimizer2)\n",
    "scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer2, 500, gamma=0.8, last_epoch=-1)\n",
    "\n",
    "sinkhorn2 = SinkhornDistance(eps=0.1, max_iter=100, device=device)\n",
    "\n",
    "tra_auc, tra_ap, tra_l, v_auc, v_ap = [], [], [], [], []\n",
    "tra_acc, tra_baseloss, tra_aux1, tra_aux2 = [], [], [], []\n",
    "train_wloss, tra_kl = [], []\n",
    "tra_r2, tra_RMSE = [], []\n",
    "\n",
    "max_r2 = 0.\n",
    "min_loss = 10.\n",
    "min_mse = 100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Epoch: 0001 base_loss= 39.31962 train_loss= 39.31962 train_r2= -3597.46452 train_RMSE= 8.86212 time= 0.03344\n",
      "--------------------------------\n",
      "Epoch: 0002 base_loss= 38.49071 train_loss= 38.49071 train_r2= -3518.96230 train_RMSE= 8.76492 time= 0.02387\n",
      "--------------------------------\n",
      "Epoch: 0003 base_loss= 33.06227 train_loss= 33.06227 train_r2= -3012.49852 train_RMSE= 8.10988 time= 0.02259\n",
      "--------------------------------\n",
      "Epoch: 0004 base_loss= 29.55837 train_loss= 29.55837 train_r2= -2677.92637 train_RMSE= 7.64644 time= 0.02306\n",
      "--------------------------------\n",
      "Epoch: 0005 base_loss= 25.55394 train_loss= 25.55394 train_r2= -2292.49586 train_RMSE= 7.07503 time= 0.02138\n",
      "--------------------------------\n",
      "Epoch: 0006 base_loss= 21.46865 train_loss= 21.46865 train_r2= -1889.92786 train_RMSE= 6.42416 time= 0.05483\n",
      "--------------------------------\n",
      "Epoch: 0007 base_loss= 20.63111 train_loss= 20.63111 train_r2= -1777.17865 train_RMSE= 6.22969 time= 0.05073\n",
      "--------------------------------\n",
      "Epoch: 0008 base_loss= 14.46234 train_loss= 14.46234 train_r2= -1174.36818 train_RMSE= 5.06484 time= 0.08289\n",
      "--------------------------------\n",
      "Epoch: 0009 base_loss= 11.76922 train_loss= 11.76922 train_r2= -877.46061 train_RMSE= 4.37865 time= 0.06979\n",
      "--------------------------------\n",
      "Epoch: 0010 base_loss= 10.05450 train_loss= 10.05450 train_r2= -661.76993 train_RMSE= 3.80330 time= 0.08791\n",
      "--------------------------------\n",
      "Epoch: 0011 base_loss= 8.34897 train_loss= 8.34897 train_r2= -443.45159 train_RMSE= 3.11452 time= 0.05172\n",
      "--------------------------------\n",
      "Epoch: 0012 base_loss= 7.89143 train_loss= 7.89143 train_r2= -341.91982 train_RMSE= 2.73574 time= 0.07241\n",
      "--------------------------------\n",
      "Epoch: 0013 base_loss= 7.46753 train_loss= 7.46753 train_r2= -260.36885 train_RMSE= 2.38839 time= 0.08167\n",
      "--------------------------------\n",
      "Epoch: 0014 base_loss= 7.66611 train_loss= 7.66611 train_r2= -252.78740 train_RMSE= 2.35350 time= 0.05373\n",
      "--------------------------------\n",
      "Epoch: 0015 base_loss= 6.77283 train_loss= 6.77283 train_r2= -163.00531 train_RMSE= 1.89194 time= 0.03502\n",
      "--------------------------------\n",
      "Epoch: 0016 base_loss= 6.77792 train_loss= 6.77792 train_r2= -171.07410 train_RMSE= 1.93792 time= 0.06479\n",
      "--------------------------------\n",
      "Epoch: 0017 base_loss= 6.73191 train_loss= 6.73191 train_r2= -188.24739 train_RMSE= 2.03233 time= 0.04542\n",
      "--------------------------------\n",
      "Epoch: 0018 base_loss= 6.19593 train_loss= 6.19593 train_r2= -169.80089 train_RMSE= 1.93074 time= 0.05694\n",
      "--------------------------------\n",
      "Epoch: 0019 base_loss= 5.79695 train_loss= 5.79695 train_r2= -166.73830 train_RMSE= 1.91335 time= 0.06584\n",
      "--------------------------------\n",
      "Epoch: 0020 base_loss= 5.17844 train_loss= 5.17844 train_r2= -143.75471 train_RMSE= 1.77744 time= 0.02892\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(20):\n",
    "    t = time.time()\n",
    "    model2.train()\n",
    "    A_pred, x1, x2, z = model2(adj_norm2, features2)\n",
    "\n",
    "    optimizer2.zero_grad()\n",
    "\n",
    "    # loss = base_loss = norm2 * F.binary_cross_entropy(A_pred.view(-1), adj_label2.view(-1), weight=weight_tensor2)\n",
    "    loss = base_loss = norm * F.mse_loss(A_pred.view(-1), adj_label2.view(-1))\n",
    "    # loss = base_loss = norm2 * torch.sqrt(F.mse_loss(A_pred.view(-1), adj_label2.view(-1)))\n",
    "    # print('#######################')\n",
    "    # print('base_loss:', loss.item())\n",
    "    tra_baseloss.append(loss.item())\n",
    "\n",
    "    kl_divergence = 0.5 / A_pred.size(0) * (\n",
    "            1 + 2 * model2.logstd - model2.mean ** 2 - torch.exp(model2.logstd) ** 2).sum(1).mean()\n",
    "    tra_kl.append(-kl_divergence.item())\n",
    "    # loss -= kl_divergence * w_w\n",
    "    loss -= kl_divergence\n",
    "\n",
    "    # wasser_loss = 0.5 / A_pred.size(0) * sinkhorn2(z, torch.randn_like(z))[0]\n",
    "    # train_wloss.append(wasser_loss.item())\n",
    "    # loss += wasser_loss * w_w\n",
    "    # loss += wasser_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "    scheduler2.step()\n",
    "\n",
    "    r2 = r2_score(adj_label2.cpu().detach().numpy().reshape(-1), A_pred.cpu().detach().numpy().reshape(-1))\n",
    "    RMSE = np.sqrt(mean_squared_error(adj_label2.cpu().detach().numpy().reshape(-1), A_pred.cpu().detach().numpy().reshape(-1)))\n",
    "    tra_r2.append(r2)\n",
    "    tra_RMSE.append(RMSE)\n",
    "\n",
    "    tra_l.append(loss.item())\n",
    "    print('--------------------------------')\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"base_loss=\", \"{:.5f}\".format(base_loss.item()),\n",
    "          \"train_loss=\", \"{:.5f}\".format(loss.item()),\n",
    "          \"train_r2=\", \"{:.5f}\".format(r2), \"train_RMSE=\", \"{:.5f}\".format(RMSE),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "    if base_loss.item() < min_loss:\n",
    "        min_loss = base_loss.item()\n",
    "        state = {'model': model2.state_dict(),\n",
    "                 'epoch': epoch,\n",
    "                 'min_loss': min_loss,\n",
    "                 }\n",
    "        torch.save(state, './sign/models/D_loss_512_256_128_5000_' + suffix + '.pth')\n",
    "model2.eval()\n",
    "\n",
    "save_dic = {'train_loss': tra_l,\n",
    "            'train_baseloss': tra_baseloss,\n",
    "            'train_aux1': tra_aux1,\n",
    "            'train_aux2': tra_aux2,\n",
    "            'train_wloss': train_wloss,\n",
    "            'train_kl': tra_kl,\n",
    "            'train_r2': tra_r2,\n",
    "            'train_RMSE': tra_RMSE,\n",
    "            }\n",
    "np.save('./sign/metrics/D_loss_512_256_128_5000_' + suffix + '.npy', save_dic)\n",
    "\n",
    "model2_ = VGAE4_2()\n",
    "model2_.to(device)\n",
    "model2_.load_state_dict(torch.load('./sign/models/D_loss_512_256_128_5000_' + suffix + '.pth')['model'])\n",
    "model2_.eval()\n",
    "out_d = model2_(adj_norm2, features2)\n",
    "\n",
    "latent_d = out_d[-1].cpu().detach().numpy()\n",
    "np.save('./sign/latent/latent_d_' + suffix + '.npy', latent_d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_zhu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
